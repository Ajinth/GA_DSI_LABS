{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ajinthchristudas/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import losses\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "Load `train.csv` from Kaggle into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "X Shape  (42000, 784)\n",
      "Y Shape  (42000,)\n"
     ]
    }
   ],
   "source": [
    "X = train[train.columns[1:]].values\n",
    "y = train['label']\n",
    "\n",
    "print (type(X))\n",
    "print (type(y))\n",
    "\n",
    "\n",
    "print ('X Shape ', X.shape)\n",
    "print ('Y Shape ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Training Digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADVCAYAAAACeWRrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b52f4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5)\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(10): \n",
    "    \n",
    "    img = X[y==i][0].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. When dealing with image data, you need to normalize your `X` by dividing each value by the max number of pixels (255).\n",
    "2. Since this is a multiclass classification problem, keras needs `y` to be a one-hot encoded matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/255.\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape  (42000, 784)\n",
      "Y Shape  (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "print ('X Shape ', X.shape)\n",
    "print ('Y Shape ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "We want to create a validation set that the model will never see to approximate how it's going to do with Kaggle's `test.csv`. Use `sklearn`'s `train_test_split` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your neural network\n",
    "\n",
    "Create a neural network using the `Dense` and `Dropout` layers from `keras`. Your activation function for the final output layer needs to be `softmax` to accomidate the ten different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/50\n",
      "33600/33600 [==============================] - 8s 251us/step - loss: 0.9713 - acc: 0.6690 - val_loss: 0.2626 - val_acc: 0.9260\n",
      "Epoch 2/50\n",
      "33600/33600 [==============================] - 8s 224us/step - loss: 0.3376 - acc: 0.9104 - val_loss: 0.1856 - val_acc: 0.9482\n",
      "Epoch 3/50\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.2548 - acc: 0.9335 - val_loss: 0.1567 - val_acc: 0.9551\n",
      "Epoch 4/50\n",
      "33600/33600 [==============================] - 7s 221us/step - loss: 0.2159 - acc: 0.9439 - val_loss: 0.1394 - val_acc: 0.9617\n",
      "Epoch 5/50\n",
      "33600/33600 [==============================] - 7s 223us/step - loss: 0.1915 - acc: 0.9511 - val_loss: 0.1299 - val_acc: 0.9632\n",
      "Epoch 6/50\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.1664 - acc: 0.9574 - val_loss: 0.1282 - val_acc: 0.9676\n",
      "Epoch 7/50\n",
      "33600/33600 [==============================] - 8s 241us/step - loss: 0.1519 - acc: 0.9609 - val_loss: 0.1258 - val_acc: 0.9655\n",
      "Epoch 8/50\n",
      "33600/33600 [==============================] - 9s 262us/step - loss: 0.1428 - acc: 0.9643 - val_loss: 0.1109 - val_acc: 0.9688\n",
      "Epoch 9/50\n",
      "33600/33600 [==============================] - 8s 250us/step - loss: 0.1312 - acc: 0.9663 - val_loss: 0.1173 - val_acc: 0.9708\n",
      "Epoch 10/50\n",
      "33600/33600 [==============================] - 7s 212us/step - loss: 0.1229 - acc: 0.9683 - val_loss: 0.1116 - val_acc: 0.9699\n",
      "Epoch 11/50\n",
      "33600/33600 [==============================] - 9s 253us/step - loss: 0.1213 - acc: 0.9696 - val_loss: 0.1042 - val_acc: 0.9725\n",
      "Epoch 12/50\n",
      "33600/33600 [==============================] - 10s 286us/step - loss: 0.1123 - acc: 0.9715 - val_loss: 0.1148 - val_acc: 0.9705\n",
      "Epoch 13/50\n",
      "33600/33600 [==============================] - 9s 257us/step - loss: 0.1074 - acc: 0.9719 - val_loss: 0.1055 - val_acc: 0.9739\n",
      "Epoch 14/50\n",
      "33600/33600 [==============================] - 7s 205us/step - loss: 0.0948 - acc: 0.9750 - val_loss: 0.1107 - val_acc: 0.9742\n",
      "Epoch 15/50\n",
      "33600/33600 [==============================] - 8s 226us/step - loss: 0.0918 - acc: 0.9754 - val_loss: 0.0971 - val_acc: 0.9750\n",
      "Epoch 16/50\n",
      "33600/33600 [==============================] - 7s 204us/step - loss: 0.0911 - acc: 0.9751 - val_loss: 0.0985 - val_acc: 0.9750\n",
      "Epoch 17/50\n",
      "33600/33600 [==============================] - 7s 205us/step - loss: 0.0828 - acc: 0.9790 - val_loss: 0.1022 - val_acc: 0.9756\n",
      "Epoch 18/50\n",
      "33600/33600 [==============================] - 7s 206us/step - loss: 0.0894 - acc: 0.9777 - val_loss: 0.0980 - val_acc: 0.9755\n",
      "Epoch 19/50\n",
      "33600/33600 [==============================] - 7s 210us/step - loss: 0.0807 - acc: 0.9789 - val_loss: 0.1148 - val_acc: 0.9720\n",
      "Epoch 20/50\n",
      "33600/33600 [==============================] - 8s 243us/step - loss: 0.0796 - acc: 0.9797 - val_loss: 0.1093 - val_acc: 0.9735\n",
      "Epoch 21/50\n",
      "33600/33600 [==============================] - 8s 239us/step - loss: 0.0763 - acc: 0.9806 - val_loss: 0.0972 - val_acc: 0.9745\n",
      "Epoch 22/50\n",
      "33600/33600 [==============================] - 8s 227us/step - loss: 0.0747 - acc: 0.9809 - val_loss: 0.0983 - val_acc: 0.9751\n",
      "Epoch 23/50\n",
      "33600/33600 [==============================] - 8s 238us/step - loss: 0.0672 - acc: 0.9826 - val_loss: 0.0992 - val_acc: 0.9760\n",
      "Epoch 24/50\n",
      "33600/33600 [==============================] - 7s 220us/step - loss: 0.0722 - acc: 0.9804 - val_loss: 0.0968 - val_acc: 0.9767\n",
      "Epoch 25/50\n",
      "33600/33600 [==============================] - 9s 279us/step - loss: 0.0680 - acc: 0.9831 - val_loss: 0.0890 - val_acc: 0.9802\n",
      "Epoch 26/50\n",
      "33600/33600 [==============================] - 10s 299us/step - loss: 0.0651 - acc: 0.9838 - val_loss: 0.0994 - val_acc: 0.9762\n",
      "Epoch 27/50\n",
      "33600/33600 [==============================] - 8s 225us/step - loss: 0.0630 - acc: 0.9833 - val_loss: 0.1006 - val_acc: 0.9771\n",
      "Epoch 28/50\n",
      "33600/33600 [==============================] - 7s 216us/step - loss: 0.0691 - acc: 0.9824 - val_loss: 0.1020 - val_acc: 0.9758\n",
      "Epoch 29/50\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0640 - acc: 0.9829 - val_loss: 0.1011 - val_acc: 0.9771\n",
      "Epoch 30/50\n",
      "33600/33600 [==============================] - 8s 244us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.1106 - val_acc: 0.9755\n",
      "Epoch 31/50\n",
      "33600/33600 [==============================] - 8s 234us/step - loss: 0.0653 - acc: 0.9844 - val_loss: 0.0994 - val_acc: 0.9788\n",
      "Epoch 32/50\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0626 - acc: 0.9843 - val_loss: 0.0981 - val_acc: 0.9773\n",
      "Epoch 33/50\n",
      "33600/33600 [==============================] - 8s 236us/step - loss: 0.0558 - acc: 0.9862 - val_loss: 0.0912 - val_acc: 0.9776\n",
      "Epoch 34/50\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0584 - acc: 0.9853 - val_loss: 0.1059 - val_acc: 0.9768\n",
      "Epoch 35/50\n",
      "33600/33600 [==============================] - 8s 237us/step - loss: 0.0528 - acc: 0.9862 - val_loss: 0.0991 - val_acc: 0.9769\n",
      "Epoch 36/50\n",
      "33600/33600 [==============================] - 8s 236us/step - loss: 0.0538 - acc: 0.9862 - val_loss: 0.0964 - val_acc: 0.9764\n",
      "Epoch 37/50\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0518 - acc: 0.9867 - val_loss: 0.1041 - val_acc: 0.9765\n",
      "Epoch 38/50\n",
      "33600/33600 [==============================] - 8s 232us/step - loss: 0.0534 - acc: 0.9863 - val_loss: 0.0988 - val_acc: 0.9777\n",
      "Epoch 39/50\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0519 - acc: 0.9869 - val_loss: 0.0973 - val_acc: 0.9787\n",
      "Epoch 40/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0482 - acc: 0.9876 - val_loss: 0.0904 - val_acc: 0.9795\n",
      "Epoch 41/50\n",
      "33600/33600 [==============================] - 8s 226us/step - loss: 0.0479 - acc: 0.9876 - val_loss: 0.0945 - val_acc: 0.9782\n",
      "Epoch 42/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0466 - acc: 0.9883 - val_loss: 0.0931 - val_acc: 0.9783\n",
      "Epoch 43/50\n",
      "33600/33600 [==============================] - 8s 229us/step - loss: 0.0405 - acc: 0.9896 - val_loss: 0.1072 - val_acc: 0.9787\n",
      "Epoch 44/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0466 - acc: 0.9885 - val_loss: 0.1003 - val_acc: 0.9786\n",
      "Epoch 45/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0469 - acc: 0.9880 - val_loss: 0.0932 - val_acc: 0.9792\n",
      "Epoch 46/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0427 - acc: 0.9888 - val_loss: 0.0993 - val_acc: 0.9788\n",
      "Epoch 47/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0439 - acc: 0.9888 - val_loss: 0.1031 - val_acc: 0.9785\n",
      "Epoch 48/50\n",
      "33600/33600 [==============================] - 8s 230us/step - loss: 0.0472 - acc: 0.9889 - val_loss: 0.1071 - val_acc: 0.9792\n",
      "Epoch 49/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0436 - acc: 0.9896 - val_loss: 0.1014 - val_acc: 0.9799\n",
      "Epoch 50/50\n",
      "33600/33600 [==============================] - 8s 228us/step - loss: 0.0420 - acc: 0.9896 - val_loss: 0.0985 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146967f28>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1], input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(int(X_train.shape[1]/2), activation='relu'))\n",
    "model.add(Dense(int(X_train.shape[1]/4), activation='relu'))\n",
    "model.add(Dense(int(X_train.shape[1]/8), activation='relu'))\n",
    "model.add(Dense(int(X_train.shape[1]/16), activation='relu'))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your model\n",
    "\n",
    "Since this is a multiclass classification problem, your loss function is `categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Done in the previous step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Use your X_test, y_test from the `train_test_split` step for the `validation_data` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Done in the Previous step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Kaggle's `test.csv`\n",
    "\n",
    "Be sure to do the **same** preprocessing you did for your training `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your predictions\n",
    "\n",
    "Use `predict_classes` to get the actual numerical values (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, 9, 3, 7, 0, 3, 0, 3, 5, 7, 4, 0, 4, 3, 3, 1, 9, 0, 9, 1, 1,\n",
       "       5, 7])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your submission\n",
    "\n",
    "1. Add your predictions to a column called `Label`\n",
    "2. You'll need to manually create the `ImageId` column, which is just a list of 1..[NUMBER OF TEST SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Label = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your submission csv\n",
    "\n",
    "Remember to set `index=False`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ImageId\": list(range(1,len(Label)+1)), \"Label\": Label}).to_csv('predictions.csv', \\\n",
    "                                                                              index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
