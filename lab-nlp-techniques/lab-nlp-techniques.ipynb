{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the needed Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Techniques Lab\n",
    "\n",
    "In this lab, we'll be practicing a set of advanced NLP techniques using tweets on airline satisfaction ([originally from Kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/data)).\n",
    "\n",
    "The first section asks you to perform LDA on the dataset to summarize the body of tweets. The second section will focus on using this data to predict the sentiment of a given tweet.\n",
    "\n",
    "Import the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/Tweets.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this data to do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use LDA to identify topics in the tweets\n",
    "\n",
    "Pick a number of topics between 5-20 and use LDA to summarize the corpus of tweets. Print out the top 25 most frequently occuring words in each topic. Do the topics appear cohesive to you? What predominant trends can you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x14770 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 133911 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(stop_words='english')\n",
    "X = df['text'].values\n",
    "X_fit = tf.fit(X)\n",
    "X_transform = X_fit.transform(X)\n",
    "X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=10, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_topics=10)\n",
    "lda.fit(X_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14770)\n"
     ]
    }
   ],
   "source": [
    "print (lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000114</th>\n",
       "      <th>000419</th>\n",
       "      <th>000ft</th>\n",
       "      <th>000lbs</th>\n",
       "      <th>0011</th>\n",
       "      <th>0016</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>...</th>\n",
       "      <th>zrh_airport</th>\n",
       "      <th>zsdgzydnde</th>\n",
       "      <th>zsuztnaijq</th>\n",
       "      <th>ztrdwv0n4l</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zv2pt6trk9</th>\n",
       "      <th>zv6cfpohl5</th>\n",
       "      <th>zvfmxnuelj</th>\n",
       "      <th>zzps5ywve2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100036</td>\n",
       "      <td>9.584669</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.988719</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.10001</td>\n",
       "      <td>0.100057</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100221</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>0.100084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100021</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10011</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500505</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.403605</td>\n",
       "      <td>0.10001</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.30622</td>\n",
       "      <td>0.63934</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100057</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.344949</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.450041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00       000    000114    000419    000ft    000lbs     0011  \\\n",
       "0  0.100036  9.584669  0.100000  0.100000  0.10000  0.100028  0.10000   \n",
       "1  4.988719  0.100024  0.100000  0.100016  0.10001  0.100057  0.10000   \n",
       "2  0.100012  0.100021  0.100000  0.100000  0.10011  0.100000  0.10000   \n",
       "3  0.100006  0.100009  0.100009  0.403605  0.10001  0.100000  0.30622   \n",
       "4  0.100010  0.100057  0.100000  0.100000  0.10000  0.100000  0.10000   \n",
       "\n",
       "      0016       00a      00am     ...      zrh_airport  zsdgzydnde  \\\n",
       "0  0.10000  0.100000  0.100006     ...         0.100000         0.1   \n",
       "1  0.10000  0.100221  0.100012     ...         0.100000         0.1   \n",
       "2  0.10000  0.100000  0.100000     ...         0.100000         0.1   \n",
       "3  0.63934  0.100000  0.100000     ...         0.100001         0.1   \n",
       "4  0.10000  0.100000  0.100001     ...         0.100000         0.1   \n",
       "\n",
       "   zsuztnaijq  ztrdwv0n4l     zukes    zurich  zv2pt6trk9  zv6cfpohl5  \\\n",
       "0    0.100014    0.100000  0.100000  0.100000    0.100000    0.100000   \n",
       "1    0.100003    0.100002  0.100006  0.100007    0.100000    0.100023   \n",
       "2    0.100000    0.100000  0.100000  0.100001    0.100000    0.100000   \n",
       "3    0.100000    0.100000  0.100000  0.100002    0.100005    0.100000   \n",
       "4    0.344949    0.100000  0.100000  0.100003    0.100000    0.100000   \n",
       "\n",
       "   zvfmxnuelj  zzps5ywve2  \n",
       "0    0.100000    0.100000  \n",
       "1    0.100025    0.100084  \n",
       "2    0.500505    0.100000  \n",
       "3    0.100010    0.100000  \n",
       "4    0.100027    0.450041  \n",
       "\n",
       "[5 rows x 14770 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Index(['reservation', 'charlotte', 'busy', 'employees', 'attendants',\n",
      "       'appreciated', 'provide', 'passenger', 'record', 'flown', 'haha',\n",
      "       'water', 'friendly', 'attitude', 'services', 'warm', '000', 'lovely',\n",
      "       'unhelpful', 'smh', 'trust', 'class', 'lie', 'level', 'upgrades'],\n",
      "      dtype='object')\n",
      "Topic 1\n",
      "Index(['americanair', 'usairways', 'flight', 'southwestair', 'united',\n",
      "       'jetblue', 'thanks', 'cancelled', 'help', 'service', 'just', 'hold',\n",
      "       'hours', 'customer', 'time', 'plane', 'flights', 'flightled', 'need',\n",
      "       'amp', 've', 'delayed', 'hour', 'phone', 'got'],\n",
      "      dtype='object')\n",
      "Topic 2\n",
      "Index(['hoping', 'yeah', 'years', 'completely', 'price', 'letting', 'control',\n",
      "       'mind', 'fare', 'round', 'depart', 'useless', 'bit', 'auto', 'btw',\n",
      "       'space', 'worth', 'center', 'select', 'south', 'web', 'oh', '2hrs',\n",
      "       'mention', 'sister'],\n",
      "      dtype='object')\n",
      "Topic 3\n",
      "Index(['info', 'booking', 'able', 'say', 'voucher', 'problems', 'send',\n",
      "       'american', 'horrible', 'email', 'united', 'app', 'message', 'haven',\n",
      "       'contact', 'reason', '800', 'instead', 'lol', 'information',\n",
      "       'americanair', 'suck', 'awful', 'lga', 'joke'],\n",
      "      dtype='object')\n",
      "Topic 4\n",
      "Index(['dca', 'extra', 'wrong', 'half', 'big', 'account', 'using', 'sat',\n",
      "       'happened', 'standby', 'thought', 'points', 'paying', 'bring', 'iad',\n",
      "       'tsa', '2nd', 'las', 'answering', 'boarded', 'month', 'running', 'mia',\n",
      "       'live', 'complete'],\n",
      "      dtype='object')\n",
      "Topic 5\n",
      "Index(['aa', 'weeks', 'heard', 'mco', 'americanairlines', 'http', 'media',\n",
      "       'country', 'luv', 'news', 'excellent', 'feb', 'wall', 'offering',\n",
      "       'phoenix', 'dropped', 'text', 'actual', 'important', 'patient',\n",
      "       'program', 'photo', 'street', 'ended', 'corporate'],\n",
      "      dtype='object')\n",
      "Topic 6\n",
      "Index(['fleet', 'http', 'card', 'wish', 'fine', 'purchase', 'atlanta',\n",
      "       'evening', 'showing', 'locator', 'dollars', 'laguardia', 'systems',\n",
      "       'bringing', 'deals', 'exit', 'elite', '150', 'honeymoon', 'rewards',\n",
      "       'premium', 'quality', 'feels', 'mileageplus', 'offers'],\n",
      "      dtype='object')\n",
      "Topic 7\n",
      "Index(['thank', 'follow', 'jetblue', 'dm', 'sure', 'airways', 'united',\n",
      "       'twitter', 'bos', 'destinationdragons', 'confirmation', 'http', 'come',\n",
      "       'return', 'im', 'feel', 'following', 'okay', 'love', 'great', 'human',\n",
      "       'lot', 'ask', 'imaginedragons', 'asap'],\n",
      "      dtype='object')\n",
      "Topic 8\n",
      "Index(['dallas', 'class', 'chicago', 'wouldn', 'happens', 'http', 'leg',\n",
      "       'start', 'offered', 'hasn', 'trouble', 'resolved', 'man',\n",
      "       'representative', 'started', 'hell', 'worry', 'overhead', 'seattle',\n",
      "       'lies', 'solution', 'music', 'columbus', 'feature', 'plenty'],\n",
      "      dtype='object')\n",
      "Topic 9\n",
      "Index(['jetblue', 'fleek', 'tell', 'answer', 'http', 'seriously', 'southwest',\n",
      "       'rt', 'point', 'stop', 'sit', 'longer', 'policy', 'fll', 'old',\n",
      "       'arrived', 'figure', 'entire', 'city', 'hang', 'seen', 'ceo', 'losing',\n",
      "       'weekend', 'gt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for topic in range(10):\n",
    "    print('Topic', topic)\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    print (word_list[0:25])\n",
    "#     print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.base.Index"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus LDA Question (Tackle if you have time / interest)\n",
    "\n",
    "Using the `.transform()` method on LDA on the data you fed it will return back a numpy array of shape `(n_rows, n_topics)`. The value in each column will identify the probability that the row in question belongs to that topic. For example, if we were looking at a row of data and an LDA model for three topics, we might see the following:\n",
    "\n",
    "```python\n",
    "lda.transform(row_of_data)\n",
    ">> [[ 0.02, 0.97, 0.01 ]]\n",
    "```\n",
    "\n",
    "This would suggest that for that row of data, it is most likely to be in the second topic (compared to the first or third topic).\n",
    "\n",
    "As a bonus challenge, try the two following questions:\n",
    "\n",
    "1. For each topic, which tweet most exemplifies (or is most likely to belong to that topic?)\n",
    "2. Find a recent tweet at an airline that you have used. Can you use the model you have currently to identify what topic does it belongs to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each topic, which tweet most exemplifies (or is most likely to belong to that topic?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the basic Vectorizer'''\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "X = df['text'].values\n",
    "X_fit = tf.fit(X)\n",
    "X_transform = X_fit.transform(X)\n",
    "\n",
    "'''Performing the LDA stuff '''\n",
    "feature_names = tf.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_topics=10)\n",
    "lda.fit(X_transform)\n",
    "\n",
    "lda_result = lda.transform(X_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of LDA Result:  <class 'numpy.ndarray'>\n",
      "[[ 0.03808219  0.03808219  0.03808219  0.03808219  0.03808219  0.44828122\n",
      "   0.03808988  0.24705178  0.03808399  0.03808219]\n",
      " [ 0.02827486  0.02827486  0.02827486  0.02827486  0.02827486  0.02827554\n",
      "   0.45925998  0.31454013  0.02827517  0.02827486]\n",
      " [ 0.02923226  0.18219171  0.02923226  0.02923226  0.02923226  0.02923406\n",
      "   0.02923631  0.58394247  0.02923306  0.02923333]\n",
      " [ 0.02382466  0.02382466  0.02382466  0.02382466  0.02382466  0.02382491\n",
      "   0.70064659  0.10875576  0.02382477  0.02382466]\n",
      " [ 0.03111752  0.03111752  0.03111752  0.03111752  0.03111752  0.03111974\n",
      "   0.03112173  0.56078201  0.03112114  0.19026781]\n",
      " [ 0.02229491  0.02229491  0.02229491  0.02229491  0.02229491  0.02229565\n",
      "   0.02229579  0.70608853  0.1155506   0.02229491]\n",
      " [ 0.24699469  0.02492648  0.02492648  0.02492648  0.0249267   0.02492726\n",
      "   0.02492816  0.55359044  0.02492682  0.02492648]\n",
      " [ 0.02463456  0.02463456  0.02463456  0.54148128  0.11488166  0.02463873\n",
      "   0.02464528  0.17117848  0.02463634  0.02463456]\n",
      " [ 0.04153647  0.04153647  0.04153647  0.04153647  0.04153647  0.04154334\n",
      "   0.04155747  0.62614085  0.04153952  0.04153647]\n",
      " [ 0.02924815  0.02924815  0.02924815  0.02924815  0.02924815  0.02924941\n",
      "   0.02925587  0.73675712  0.02924871  0.02924815]]\n"
     ]
    }
   ],
   "source": [
    "print ('Type of LDA Result: ', type(lda_result))\n",
    "print (lda_result[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>Topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.038082</td>\n",
       "      <td>0.448281</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.247052</td>\n",
       "      <td>0.038084</td>\n",
       "      <td>0.038082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028276</td>\n",
       "      <td>0.459260</td>\n",
       "      <td>0.314540</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.028275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029232</td>\n",
       "      <td>0.182192</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>0.029234</td>\n",
       "      <td>0.029236</td>\n",
       "      <td>0.583942</td>\n",
       "      <td>0.029233</td>\n",
       "      <td>0.029233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.700647</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031118</td>\n",
       "      <td>0.031118</td>\n",
       "      <td>0.031118</td>\n",
       "      <td>0.031118</td>\n",
       "      <td>0.031118</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.560782</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>0.190268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic1    Topic2    Topic3    Topic4    Topic5    Topic6    Topic7  \\\n",
       "0  0.038082  0.038082  0.038082  0.038082  0.038082  0.448281  0.038090   \n",
       "1  0.028275  0.028275  0.028275  0.028275  0.028275  0.028276  0.459260   \n",
       "2  0.029232  0.182192  0.029232  0.029232  0.029232  0.029234  0.029236   \n",
       "3  0.023825  0.023825  0.023825  0.023825  0.023825  0.023825  0.700647   \n",
       "4  0.031118  0.031118  0.031118  0.031118  0.031118  0.031120  0.031122   \n",
       "\n",
       "     Topic8    Topic9   Topic10  \n",
       "0  0.247052  0.038084  0.038082  \n",
       "1  0.314540  0.028275  0.028275  \n",
       "2  0.583942  0.029233  0.029233  \n",
       "3  0.108756  0.023825  0.023825  \n",
       "4  0.560782  0.031121  0.190268  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ['Topic'+ str(i+1) for i in range (10)]\n",
    "\n",
    "results = pd.DataFrame(lda_result, columns=topics)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Shape:  (14640, 10)\n",
      "Core DF Shape:  (14640, 15)\n"
     ]
    }
   ],
   "source": [
    "print ('Results Shape: ' , results.shape)\n",
    "print ('Core DF Shape: ' , df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([76, 388], dtype='int64')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('Topic1', ascending=False).index[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a recent tweet at an airline that you have used. Can you use the model you have currently to identify what topic does it belongs to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topic1': 76,\n",
      " 'Topic10': 855,\n",
      " 'Topic2': 1851,\n",
      " 'Topic3': 3073,\n",
      " 'Topic4': 8164,\n",
      " 'Topic5': 12463,\n",
      " 'Topic6': 1227,\n",
      " 'Topic7': 253,\n",
      " 'Topic8': 826,\n",
      " 'Topic9': 87}\n",
      "{'Topic1': '@VirginAmerica Or watch some of the best student films in the '\n",
      "           'country at 35,000 feet! #CMFat35000feet http://t.co/KEK5pDMGiF',\n",
      " 'Topic10': '@united http://t.co/hj5kq82Chn, however, is completely under your '\n",
      "            'control—the price was and still is displayed on '\n",
      "            'http://t.co/hj5kq82Chn.',\n",
      " 'Topic2': '@united This is probably the least dependable airline in the '\n",
      "           'Western Hemisphere. @united does not belong in Star Alliance, but '\n",
      "           'SkyTeam',\n",
      " 'Topic3': '@united  oh united, how much I despise thee!',\n",
      " 'Topic4': '@JetBlue 108 to Portland Maine',\n",
      " 'Topic5': '@AmericanAir Super Spring Tides and “Tide of The Century” Drawing '\n",
      "           'tourists to French and U.K coasts:\\n'\n",
      "           'http://t.co/gXdqORtsS0',\n",
      " 'Topic6': '@United to operated #B767-300ER from #Newark to #Zurich '\n",
      "           '@ZRH_airport replacing B767-400ER between 6MAY-23SEP instead till '\n",
      "           '23OCT #avgeek',\n",
      " 'Topic7': \"@VirginAmerica If you'd love to see more girls be inspired to \"\n",
      "           'become pilots, RT our free WOAW event March 2-8 at ABQ. '\n",
      "           'http://t.co/rfXlV1kGDh',\n",
      " 'Topic8': '@united UA938 ORD-LHR.  bags are being loaded 30min Late Flight. '\n",
      "           'Frigid air into cabin! Plane feels like falling apart! Upgrade '\n",
      "           'long haul fleet!',\n",
      " 'Topic9': \"@VirginAmerica Random Q: what's the distribution of elevate \"\n",
      "           'avatars? I bet that kitty has a disproportionate share '\n",
      "           'http://t.co/APtZpuROp4'}\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "\n",
    "topic_dict = {}\n",
    "topic_detail_dict = {}\n",
    "\n",
    "for i in range(1,11): \n",
    "    new_i = 'Topic'+ str(i)\n",
    "    topic_dict[new_i] = results.sort_values(new_i, ascending=False).index[0]\n",
    "    topic_detail_dict[new_i] = df.iloc[results.sort_values(new_i, ascending=False).index[0]]['text']\n",
    "    \n",
    "\n",
    "    \n",
    "pprint.pprint (topic_dict)\n",
    "pprint.pprint (topic_detail_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use NLP to predict the sentiment of tweets\n",
    "\n",
    "In this section, please use any of the NLP techniques that we have covered over the last two days to best predict whether a tweet has a negative sentiment or not. Transformation code for your target variable is below.\n",
    "\n",
    "**Bonus Consideration**: Outside of the text itself, do other factors in the dataset have an effect? Do your results change if you include features like the airline or the timezone of the tweet?\n",
    "\n",
    "Don't forget to create a training and test set to compare your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['negative'] = df['airline_sentiment'].apply(lambda x: 1 if x =='negative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.6s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.7s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.7s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.5s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.7s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.6s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.7s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.7s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.7s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.8s\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.7s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   1.5s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   0.7s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   0.6s\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.9s\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.7s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   0.6s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   10.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   0.8s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   1.1s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.7s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'logreg__C': 10, 'logreg__max_iter': 100, 'logreg__penalty': 'l2'}\n",
      "Best Score:  0.820782103825\n",
      "Train Score:  0.961919398907\n",
      "Test Score:  0.814890710383\n",
      "Confusion Matrix on Train \n",
      "\n",
      "[[4086  288]\n",
      " [ 158 7180]]\n",
      "Classification Report on Train\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.95      4374\n",
      "          1       0.96      0.98      0.97      7338\n",
      "\n",
      "avg / total       0.96      0.96      0.96     11712\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix on Test \n",
      "\n",
      "[[ 792  296]\n",
      " [ 246 1594]]\n",
      "\n",
      "\n",
      "Classification Report on Test\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.73      0.75      1088\n",
      "          1       0.84      0.87      0.85      1840\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the Targets and Features '''\n",
    "\n",
    "\n",
    "X = df['text']\n",
    "y = df['negative']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "\n",
    "'''Setting up the TFIDVectorizer'''\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "'''Intantiating Logreg'''\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "'''Setting up the Parameters for GridSearch'''\n",
    "params = {\n",
    "\n",
    "    'logreg__penalty': ['l1', 'l2'], \n",
    "    'logreg__C': [1.0,10,100], \n",
    "    'logreg__max_iter': [100,150,200]   \n",
    "}\n",
    "\n",
    "'''Setting the Pipeline'''\n",
    "logreg_tk_pipe = Pipeline([('vect', tf), \n",
    "                     ('logreg', logreg)])\n",
    "\n",
    "'''Fitting the Model on Training'''\n",
    "gs_logreg = GridSearchCV(logreg_tk_pipe, param_grid=params,n_jobs = -1, verbose=2, scoring='accuracy')\n",
    "gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print ('Best Params: ' , gs_logreg.best_params_) \n",
    "print ('Best Score: ', gs_logreg.best_score_)\n",
    "print ('Train Score: ',gs_logreg.score(X_train, y_train))\n",
    "print ('Test Score: ', gs_logreg.score(X_test, y_test))\n",
    "print ('Confusion Matrix on Train \\n')\n",
    "print (confusion_matrix(y_train, gs_logreg.predict(X_train)))\n",
    "print ('Classification Report on Train\\n')\n",
    "print (classification_report(y_train, gs_logreg.predict(X_train)))\n",
    "print ('\\n')\n",
    "print ('Confusion Matrix on Test \\n')\n",
    "print (confusion_matrix(y_test, gs_logreg.predict(X_test)))\n",
    "print ('\\n')\n",
    "print ('Classification Report on Test\\n')\n",
    "print (classification_report(y_test, gs_logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
