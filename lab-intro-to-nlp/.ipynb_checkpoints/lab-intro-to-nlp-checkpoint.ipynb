{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Needed Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP Lab\n",
    "\n",
    "In this lab, you'll be classifying randomly selected tweets from political officials into whether or not they are partisan tweets or neutral. In the following import statement, we're selecting only the columns that are important, but there may be more useful features in that set. Feel free to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias                                               text\n",
       "0  partisan  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...\n",
       "1  partisan  VIDEO - #Obamacare:  Full of Higher Costs and ...\n",
       "2   neutral  Please join me today in remembering our fallen...\n",
       "3   neutral  RT @SenatorLeahy: 1st step toward Senate debat...\n",
       "4  partisan  .@amazon delivery #drones show need to update ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/political_media.csv',\n",
    "                usecols=[7, 20])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3689\n",
       "partisan    1311\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bias.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['bias'] = df['bias'].apply(lambda x: 1 if x =='partisan' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3689\n",
       "1    1311\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bias.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text']\n",
    "y = df['bias']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Please try the following techniques to transform the data. For each technique, do the following:\n",
    "\n",
    "1. Transform the training data\n",
    "2. Fit a `RandomForestClassifier` to the transformed training data\n",
    "3. Transform the test data\n",
    "4. Discuss the goodness of fit of your model using the test data and a classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `CountVectorizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.975223880597\n",
      "Test Score:  0.721818181818\n",
      "[[1146   55]\n",
      " [ 404   45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.95      0.83      1201\n",
      "          1       0.45      0.10      0.16       449\n",
      "\n",
      "avg / total       0.66      0.72      0.65      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "cv = CountVectorizer()\n",
    "X_fit = cv.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the training set seems to perform better but the test seem does not seem to perform as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `CountVectorizer()` with your choice of `min_df` and `max_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.939104477612\n",
      "Test Score:  0.703636363636\n",
      "[[1082  119]\n",
      " [ 370   79]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.90      0.82      1201\n",
      "          1       0.40      0.18      0.24       449\n",
      "\n",
      "avg / total       0.65      0.70      0.66      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "cv = CountVectorizer(min_df=0.10, max_df=0.90)\n",
    "X_fit = cv.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the training set seems to perform better but the test seem does not seem to perform as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `CountVectorizer()` with English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.971940298507\n",
      "Test Score:  0.74\n",
      "[[1146   55]\n",
      " [ 374   75]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.95      0.84      1201\n",
      "          1       0.58      0.17      0.26       449\n",
      "\n",
      "avg / total       0.71      0.74      0.68      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X_fit = cv.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `TfidfVectorizer()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.974626865672\n",
      "Test Score:  0.726666666667\n",
      "[[1145   56]\n",
      " [ 395   54]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.95      0.84      1201\n",
      "          1       0.49      0.12      0.19       449\n",
      "\n",
      "avg / total       0.67      0.73      0.66      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "tf = TfidfVectorizer()\n",
    "X_fit = tf.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `TfidfVectorizer()` with English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.973432835821\n",
      "Test Score:  0.750303030303\n",
      "[[1148   53]\n",
      " [ 359   90]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.96      0.85      1201\n",
      "          1       0.63      0.20      0.30       449\n",
      "\n",
      "avg / total       0.73      0.75      0.70      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "X_fit = tf.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "rf_fit = rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving forward\n",
    "\n",
    "With the remainder of your time, please try and find the best model and data transformation to predict partisan tweets. This is a challenging data set and can be approached from a number of ways.\n",
    "\n",
    "Some techniques to try are:\n",
    "\n",
    "1. Different types of data transformation \n",
    "2. Custom preprocessors for `CountVectorizer`\n",
    "3. Custom stopword lists\n",
    "4. Use of a dimensionality reduction technique (like `TruncatedSVD`)\n",
    "5. Optimizing hyperparameters using `GridSearchCV`\n",
    "6. Trying a different modeling technique such as `KNeighborsClassifier` or `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing out the Model with the RandomClassifier Apporach. We also tried to hypertune some of the Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=50 ....................\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=50 ....................\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=50 ....................\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=100 ...................\n",
      "[CV] ..... criterion=gini, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] ..... criterion=gini, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=100 ...................\n",
      "[CV] ..... criterion=gini, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=150 ...................\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=100 ...................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=100, total=   0.5s\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=100, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=4, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=100, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=50 ....................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=150, total=   0.7s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=50 ....................\n",
      "[CV] ..... criterion=gini, max_depth=6, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=50 ....................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=100 ...................\n",
      "[CV] ..... criterion=gini, max_depth=6, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=100 ...................\n",
      "[CV] ..... criterion=gini, max_depth=6, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=100 ...................\n",
      "[CV] .... criterion=gini, max_depth=4, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=100, total=   0.6s\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=100, total=   0.6s\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=150 ...................\n",
      "[CV] criterion=gini, max_depth=6, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=100, total=   0.5s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=50 ....................\n",
      "[CV] ..... criterion=gini, max_depth=8, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=50 ....................\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=50 ....................\n",
      "[CV] ..... criterion=gini, max_depth=8, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=100 ...................\n",
      "[CV] ..... criterion=gini, max_depth=8, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=100 ...................\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=150, total=   0.9s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=100 ...................\n",
      "[CV] .... criterion=gini, max_depth=6, n_estimators=150, total=   1.0s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=100, total=   0.8s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=100, total=   0.8s\n",
      "[CV] criterion=gini, max_depth=8, n_estimators=150 ...................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=100, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=50 .................\n",
      "[CV] .. criterion=entropy, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=50 .................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=150, total=   1.1s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=50 .................\n",
      "[CV] .. criterion=entropy, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=100 ................\n",
      "[CV] .. criterion=entropy, max_depth=4, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=100 ................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=150, total=   1.1s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=100 ................\n",
      "[CV] .... criterion=gini, max_depth=8, n_estimators=150, total=   1.1s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=100, total=   0.5s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=100, total=   0.5s\n",
      "[CV] criterion=entropy, max_depth=4, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=100, total=   0.5s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=50 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. criterion=entropy, max_depth=6, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=50 .................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=50 .................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=100 ................\n",
      "[CV] . criterion=entropy, max_depth=4, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=100 ................\n",
      "[CV] .. criterion=entropy, max_depth=6, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=100 ................\n",
      "[CV] .. criterion=entropy, max_depth=6, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=100, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=100, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=6, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=100, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=50 .................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=150, total=   0.9s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=50 .................\n",
      "[CV] .. criterion=entropy, max_depth=8, n_estimators=50, total=   0.3s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=50 .................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=150, total=   0.9s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=100 ................\n",
      "[CV] .. criterion=entropy, max_depth=8, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=100 ................\n",
      "[CV] . criterion=entropy, max_depth=6, n_estimators=150, total=   0.8s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=100 ................\n",
      "[CV] .. criterion=entropy, max_depth=8, n_estimators=50, total=   0.4s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=100, total=   0.6s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=100, total=   0.6s\n",
      "[CV] criterion=entropy, max_depth=8, n_estimators=150 ................\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=100, total=   0.7s\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=150, total=   0.8s\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=150, total=   0.7s\n",
      "[CV] . criterion=entropy, max_depth=8, n_estimators=150, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [50, 100, 150], 'criterion': ['gini', 'entropy'], 'max_depth': [4, 6, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Setting up the CountVectoriser'''\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "X_fit = tf.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [50,100,150], \n",
    "    'criterion' : ['gini', 'entropy'], \n",
    "    'max_depth': [4,6,8]\n",
    "    \n",
    "}\n",
    "\n",
    "'''Fitting the RandomForestClassifier Model on Training'''\n",
    "rf = RandomForestClassifier()\n",
    "gv_rf = GridSearchCV(rf, param_grid=params,n_jobs = -1, verbose=2)\n",
    "gv_rf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Best Score:  0.742686567164\n"
     ]
    }
   ],
   "source": [
    "print ('Best Estimator: ', gv_rf.best_estimator_)\n",
    "print ('Best Score: ', gv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.742686567164\n",
      "Test Score:  0.727878787879\n",
      "[[1201    0]\n",
      " [ 449    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      1.00      0.84      1201\n",
      "          1       0.00      0.00      0.00       449\n",
      "\n",
      "avg / total       0.53      0.73      0.61      1650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "'''Validating the score on Training and Test Sets'''\n",
    "train_rf_score = gv_rf.score(X_train_transformed, y_train)\n",
    "print ('Training Score: ', train_rf_score)\n",
    "\n",
    "test_rf_score = gv_rf.score(X_test_transformed, y_test)\n",
    "print ('Test Score: ', test_rf_score)\n",
    "\n",
    "\n",
    "'''Model Evaluation'''\n",
    "\n",
    "# Confusion Matrix \n",
    "conf_matrix = confusion_matrix(y_test, gv_rf.predict(X_test_transformed))\n",
    "print (conf_matrix)\n",
    "\n",
    "# Classification Report \n",
    "class_report = classification_report(y_test, gv_rf.predict(X_test_transformed))\n",
    "print (class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.742686567164\n",
      "Test Score:  0.727878787879\n",
      "[[1201    0]\n",
      " [ 449    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      1.00      0.84      1201\n",
      "          1       0.00      0.00      0.00       449\n",
      "\n",
      "avg / total       0.53      0.73      0.61      1650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search and Pipeline with Logisitic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.2s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.2s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.2s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.5s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.5s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.2s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.2s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   0.4s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.4s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   0.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.4s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   9.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=  12.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=  13.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=  13.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   0.8s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   18.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   0.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   0.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   0.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   0.4s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.6s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.3s\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=  18.2s\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=  16.7s\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=  19.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   34.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'logreg__penalty': ['l1', 'l2'], 'logreg__C': [1.0, 10, 100], 'logreg__max_iter': [100, 150, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Setting up the TFIDVectorizer'''\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "X_fit = tf.fit(X_train)\n",
    "\n",
    "X_train_transformed = X_fit.transform(X_train)\n",
    "X_test_transformed = X_fit.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "'''Setting up the Parameters for GridSearch'''\n",
    "params = {\n",
    "    \n",
    "#     'vect__ngram_range': [(1,1)],\n",
    "    'logreg__penalty': ['l1', 'l2'], \n",
    "    'logreg__C': [1.0,10,100], \n",
    "    'logreg__max_iter': [100,150,200]   \n",
    "}\n",
    "\n",
    "'''Setting the Pipeline'''\n",
    "logreg_tk_pipe = Pipeline([('vect', tf), \n",
    "                     ('logreg', logreg)])\n",
    "\n",
    "'''Fitting the Model on Training'''\n",
    "gs_logreg = GridSearchCV(logreg_tk_pipe, param_grid=params,n_jobs = -1, verbose=2, scoring='accuracy')\n",
    "gs_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99432835820895527"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75272727272727269"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
