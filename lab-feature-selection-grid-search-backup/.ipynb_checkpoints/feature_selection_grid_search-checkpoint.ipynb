{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Today's dataset comes from the UCI Machine Learning Repository and is a set of data on [Online News Popularity](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) for articles shared on Mashable.\n",
    "\n",
    "The dataset can be found in this repository (`datasets/OnlineNewsPopularity.csv`) and the codebook can be found within the `datasets` folder as well (or online [here](https://git.generalassemb.ly/DSI-EAST-1/lab-feature-selection-grid-search/blob/master/datasets/OnlineNewsPopularity.names).) However, you will not need to have intimate domain knowledge to create a great model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: EDA\n",
    "\n",
    "For this step, do the following:\n",
    "- Import the data in `datasets/OnlineNewsPopularity.csv`\n",
    "- Use methods like `.describe()` and `.info()` to identify if there is any missing data and any non-numeric data\n",
    "- Print out the results of `df.columns` -- are these names formatted correctly?\n",
    "- Drop any columns that do not contain numeric values\n",
    "- Drop or impute any missing data\n",
    "- Recode the `shares` column (your target) to be 1 if shares is >= 1500 and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Correlation Table\n",
    "\n",
    "Use Seaborn and pandas to create a correlation heatmap. What patterns are apparent to you? Is this easy to use or too cluttered to make sense of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-test split\n",
    "\n",
    "Split your data frame into your target (the `shares` column) and your features (all other columns), then split those into a training and holdout (test) set. You will not use this holdout set until the end of the lab. \n",
    "\n",
    "Your test size should be `0.33` and should use the random seed `20170825`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection through Regularization\n",
    "\n",
    "Use a Lasso Logistic Regression to decide on a set of features to select. \n",
    "\n",
    "1. Standardize your training dataset.\n",
    "2. Fit three lasso logistic regressions with `C` of 0.1, 1.0, and 10.0. Double check with the documentation to ensure that you are fitting **Lasso** logistic regressions.\n",
    "3. Identify which columns have zero coefficients. \n",
    "4. What trends do you see? Are there any features that seem like strong contenders to be put in the model? \n",
    "    > If there are no trends yet, no worries! We're going to try other feature selection techniques as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection with RFE\n",
    "\n",
    "Use Recursive Feature Elimination and a new Logistic Regression object to do feature selection.\n",
    "\n",
    "1. Instantiate an `RFE` object, set to cut down your input features to 50% of the input, as well as a `LogisticRegression` object.\n",
    "2. Fit your `RFE` object to your standardized features in your training set. \n",
    "3. What features does RFE suggest to drop? Are there any similarities to those features dropped by regularization?\n",
    "    > Remember that after fitting, `RFE.support_` gives you a mask of columns, where columns marked `True` are going to be kept and columns marked `False` should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection with SelectKBest\n",
    "\n",
    "Use SelectKBest and `f_classif` to select the top 30 features\n",
    "\n",
    "1. Instantiate a SelectKBest, using a `score_func` of `f_classif` and a `k` of 30\n",
    "    > Note, `f_classif` needs to be imported as well! \n",
    "2. Fit SelectKBest to your standardized features in the training set.\n",
    "3. Print out which columns are dropped in this case.\n",
    "4. Pick a set of columns (such as `LDA`, `polarity`, `weekday`, or `data_channel` -- are any columns that have been consistently dropped? \n",
    "    > Remember that after fitting, `SelectKBest.get_support()` gives you a mask of columns, where columns marked `True` are going to be kept and columns marked `False` should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Choosing a Feature Set\n",
    "\n",
    "1. Choose a set of features to keep:\n",
    "     - Features with non-zero coefficients from regularization\n",
    "     - Features kept by RFE\n",
    "     - Features kept by SelectKBest\n",
    "2. Create a new version of your training and holdout features. Drop the columns you will not be keeping from these new versions of your training and holdout features. **Do not overwrite your original training and holdout features** -- we will use those shortly.  Print out the shapes to ensure that you have successfully dropped those columns. \n",
    "    > Remember that both `RFE` and `SelectKBest` give you a mask of columns to keep and that those columns are in the same order in both your training and holdout sets!\n",
    "3. Create a correlation heatmap with your reduced feature set. Do you see any patterns? What type of features seem important? Do they relate to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Optimization with GridSearch\n",
    "\n",
    "Fit three logistic regressions on your training set, with the following parameters:\n",
    "\n",
    "1. A logistic regression with the default hyperparameters, with all features.\n",
    "2. A logistic regression with the default hyperparameters, with only the features you selected in question 7.\n",
    "3. A logistic regression using `GridSearchCV` to optimize the following hyperparameters, with only the features you selected in question 7. Print out the hyperparameters selected by `GridSearchCV`\n",
    "  - `penalty`: should we use l1 or l2 regularization?\n",
    "  - `C`: how strong should our regularization be?\n",
    "4. Score each of your fit models against the holdout set. Which model does the best?\n",
    "5. Print out a confusion matrix for each model. Are there any trends that you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. _Bonus_ Continue Optimizing!\n",
    "\n",
    "In this lab, you're not expected to complete all (or even any) of question 9. However, if you find yourself with extra time, any of these options may lead to a higher score. Modeling is complex and often has a *number* of pathways to go down. This is a small selection  of potential next steps for this dataset.\n",
    "\n",
    "We have only scratched the surface of how to optimize a model for this dataset. With your remaining time, feel free to optimize in one of a few different ways, making sure that you are using cross-validation to check that your decisions are sound:\n",
    "\n",
    "1. **Feature Selection**: try a model with a new set of features from the following methods:\n",
    "    - A higher regularization strength for feature selection via regularization\n",
    "    - A lower / higher number of features returned by `RFE` or `SelectKBest`\n",
    "2. **Feature Engineering**: try a model with the following tweaks:\n",
    "    - Are all the features modeled correctly? Should we create dummy variables or otherwise transform any of the current features?\n",
    "    - We discarded the `url` feature very early onwards -- is there useful information we could extract from there?\n",
    "    - Is our decision to predict over/under 1500 shares correct? Do EDA on the original `shares` column to identify if there is a better cutoff.\n",
    "3. **Model Selection**: \n",
    "    - Try a different classification model such as _k_-Nearest Neighbors\n",
    "    - Is classification even a correct choice here? Could we use a regression model to predict the actual number of shares instead?\n",
    "4. **Hyperparameter Optimization**: \n",
    "    - Continue doing hyperparameter optimization through `GridSearchCV`\n",
    "    - **Double Bonus**: [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) performs the same parametric search that `GridSearchCV` does, but over a random distribution of hyperparameters (versus user-selected ones). This can be a helpful way to start zeroing in on an area where you may want to try some more targeted grid searching\n",
    "    - **Triple Bonus**: `GridSearchCV` automatically scores and ranks results based on the default scoring method in each model type ($R^2$ for `LinearRegression`, mean accuracy for `KNeighborsClassifier`, etc.) We can direct `GridSearchCV` to prioritize different metrics, however, such as precision or recall for class 1 or class 0, for example. Do some independent research on the [scoring parameter in sklearn](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) and try to implement it yourself. Pick two or three metrics and optimize for each. Are the hyperparameters chosen different across each case?\n",
    "    > **Note**: this is pretty advanced. Try this only if you're _exceptionally_ comfortable with the material this week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
